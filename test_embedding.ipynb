{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da3da322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import builtins\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "sys.path.extend(['..', '.'])\n",
    "from datasets.dataset_tinyimagenet import load_train, load_val_loader, num_classes_dict\n",
    "from tools.store import ExperimentLogWriter\n",
    "import models.builder as model_builder\n",
    "import utils\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "model_names += ['resnet18_cifar_variant1']\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    dataset='cifar10',\n",
    "    arch='resnet18_cifar_variant1',\n",
    "    workers=32,\n",
    "    epochs=100,\n",
    "    start_epoch=0,\n",
    "    batch_size=256,\n",
    "    lr=30.0,\n",
    "    schedule=[60, 80],\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0,\n",
    "    print_freq=10,\n",
    "    evaluate=False,\n",
    "    world_size=-1,\n",
    "    rank=-1,\n",
    "    dist_url='tcp://224.66.41.62:23456',\n",
    "    dist_backend='nccl',\n",
    "    seed=None,\n",
    "    gpu=None,\n",
    "    multiprocessing_distributed=False,\n",
    "    opt='sgd',\n",
    "    dir='log/spectral/completed-2023-05-13spectral-resnet18-mlp1000-norelu-cifar10-lr003-mu1-log_freq:20',\n",
    "    num_per_class=int(1e10),\n",
    "    val_every=5,\n",
    "    latest_only=True,\n",
    "    mpd=False,\n",
    "    dist_url_add=0,\n",
    "    specific_ckpts=None,\n",
    "    use_random_labels=False,\n",
    "    normalize=False,\n",
    "    nomlp=True,\n",
    "    aug='standard'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8dfb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     args = parser.parse_args()\n",
    "    if args.mpd:\n",
    "        args.multiprocessing_distributed = True\n",
    "        args.world_size = 1\n",
    "        args.rank = 0\n",
    "        args.dist_url = 'tcp://127.0.0.1:' + str(10001 + args.dist_url_add)\n",
    "    utils.spawn_processes(main_worker, args)\n",
    "\n",
    "\n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    global best_acc1\n",
    "    args.gpu = gpu\n",
    "    # suppress printing if not master\n",
    "    if args.multiprocessing_distributed and args.gpu != 0:\n",
    "        def print_pass(*args):\n",
    "            pass\n",
    "        builtins.print = print_pass\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "\n",
    "    if args.distributed:\n",
    "        utils.init_proc_group(args, ngpus_per_node)\n",
    "    \n",
    "    logger = ExperimentLogWriter(args.dir)\n",
    "\n",
    "    # loop through checkpoints and set pre-trained\n",
    "    ckpt_dir = os.path.join(args.dir, 'checkpoints')\n",
    "    for fname in sorted(os.listdir(ckpt_dir)):\n",
    "        if args.latest_only and not fname.startswith('latest_'): continue\n",
    "        if args.specific_ckpts is not None and fname not in args.specific_ckpts: continue\n",
    "        args.pretrained = os.path.join(ckpt_dir, fname)\n",
    "\n",
    "        lineval_dir = os.path.join(args.dir, 'lin_eval_ckpt')\n",
    "        if os.path.exists(lineval_dir):\n",
    "            print('linear evaluation dir exists at {}, may overwrite...'.format(lineval_dir))\n",
    "        eval_ckpt(\n",
    "            copy.deepcopy(args), # because args.batch_size and args.workers are changed\n",
    "            ngpus_per_node,\n",
    "            fname,\n",
    "            logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ec1eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, dataloader, device):\n",
    "    # Print all layers in the original model\n",
    "    print(\"Original model layers:\")\n",
    "    for idx, layer in enumerate(model.children()):\n",
    "        print(f\"Layer {idx}: {layer}\")\n",
    "    print()\n",
    "\n",
    "    # Copy the model without its last layer\n",
    "    embedding_model = nn.Sequential(*(list(model.children())[:-1]))\n",
    "    embedding_model = embedding_model.to(device)\n",
    "\n",
    "    # Print all layers in the embedding model\n",
    "    print(\"Embedding model layers:\")\n",
    "    for idx, layer in enumerate(embedding_model.children()):\n",
    "        print(f\"Layer {idx}: {layer}\")\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    embedding_model.eval()\n",
    "\n",
    "    # Initialize an empty tensor to store all the embeddings\n",
    "    all_embeddings = torch.empty((0, 512)).to(device)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            print(f\"{images.shape = }\")\n",
    "\n",
    "            # Get embeddings for this batch and flatten them\n",
    "            print(f\"{embedding_model(images).shape = }\")\n",
    "            embeddings = embedding_model(images).view(images.size(0), -1)\n",
    "            \n",
    "            print(f\"{embeddings.shape = }\")\n",
    "\n",
    "            # Concatenate with the previous embeddings\n",
    "            all_embeddings = torch.cat((all_embeddings, embeddings))\n",
    "\n",
    "    return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4be5c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ckpt(args, ngpus_per_node, ptrain_fname, logger):\n",
    "    # create model\n",
    "    pretrained_id = ptrain_fname.split('.')[0]\n",
    "    dict_id = pretrained_id + '_lineval'\n",
    "    dict_id += '_{}_lr:{}_wd:{}_{}eps'.format(args.opt, args.lr, args.weight_decay, args.epochs)\n",
    "    if args.nomlp:\n",
    "        dict_id = dict_id + '_nomlp'\n",
    "    dict_id += '_aug:' + args.aug\n",
    "    dict_id += '_random_labels' if args.use_random_labels else ''\n",
    "    ckpt_dir = os.path.join(args.dir, 'lin_eval_ckpt')\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    ptrain_fname += '_random_labels' if args.use_random_labels else ''\n",
    "    lin_eval_loc = os.path.join(ckpt_dir, ptrain_fname)\n",
    "\n",
    "    logger.create_data_dict(\n",
    "        ['epoch', 'train_acc', 'val_acc','train_loss', 'val_loss', 'train5', 'val5'],\n",
    "        dict_id=dict_id)\n",
    "\n",
    "    model = model_builder.get_model(num_classes_dict[args.dataset], arch=args.arch)\n",
    "\n",
    "    # freeze all layers but the last fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in ['fc.weight', 'fc.bias']:\n",
    "            param.requires_grad = False\n",
    "    # init the fc layer\n",
    "    model.fc.weight.data.normal_(mean=0.0, std=0.01)\n",
    "    model.fc.bias.data.zero_()\n",
    "\n",
    "    # load from pre-trained, before DistributedDataParallel constructor\n",
    "    if args.pretrained:\n",
    "        if os.path.isfile(args.pretrained):\n",
    "            checkpoint = torch.load(args.pretrained, map_location='cpu')\n",
    "            state_dict = checkpoint['state_dict']\n",
    "            model_builder.load_checkpoint(model, state_dict, args.pretrained, args=args, nomlp=args.nomlp)\n",
    "\n",
    "            # Get the last layer\n",
    "            last_layer_name, last_layer = list(model.named_children())[-1]\n",
    "            print(f\"Last layer: {last_layer_name}\")\n",
    "            print(f\"Last layer's shape: {last_layer.weight.shape}\")\n",
    "\n",
    "            ###########################################\n",
    "            # Output:\n",
    "            # Last layer: fc\n",
    "            # Last layer's shape: torch.Size([10, 512])\n",
    "            ###########################################\n",
    "\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.pretrained))\n",
    "    \n",
    "    model = utils.init_data_parallel(args, model, ngpus_per_node)\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n",
    "\n",
    "    # optimize only the linear classifier\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    assert len(parameters) == 2  # fc.weight, fc.bias\n",
    "    if args.opt=='sgd':\n",
    "        optimizer = torch.optim.SGD(parameters, args.lr,\n",
    "                                    momentum=args.momentum,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "    elif args.opt=='adam':\n",
    "        optimizer = torch.optim.Adam(parameters, lr=args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=args.weight_decay)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    if args.use_random_labels:\n",
    "        random_labels = torch.load(os.path.join(args.dir, 'saved_tensors', 'random_labels.pth')).numpy()\n",
    "    else:\n",
    "        random_labels = None\n",
    "    train_sampler, train_loader = load_train(args.dataset, args.num_per_class, args.distributed,\n",
    "                                             args.batch_size, args.workers, data_aug=args.aug, random_labels=random_labels)\n",
    "\n",
    "    embeddings = get_embeddings(model, train_loader, args.gpu)\n",
    "#     print(f\"Embeddings shape: {embeddings.shape}\")  # Should be [60000, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a7f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7803dadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear evaluation dir exists at log/spectral/completed-2023-05-13spectral-resnet18-mlp1000-norelu-cifar10-lr003-mu1-log_freq:20/lin_eval_ckpt, may overwrite...\n",
      "=> loading checkpoint 'log/spectral/completed-2023-05-13spectral-resnet18-mlp1000-norelu-cifar10-lr003-mu1-log_freq:20/checkpoints/latest_800.pth'\n",
      "=> loaded pre-trained model 'log/spectral/completed-2023-05-13spectral-resnet18-mlp1000-norelu-cifar10-lr003-mu1-log_freq:20/checkpoints/latest_800.pth'\n",
      "Last layer: fc\n",
      "Last layer's shape: torch.Size([10, 512])\n",
      "Original model layers:\n",
      "Layer 0: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Embedding model layers:\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9765c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spectral",
   "language": "python",
   "name": "spectral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
